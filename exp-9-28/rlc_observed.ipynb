{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO observe RLC, for now, use uncontainzerized version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script setup the experiments, assuming that the l4s part has been installed correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "tx0_prefix = \"ssh PeterYao@pc490.emulab.net\"\n",
    "router0_prefix = \"ssh PeterYao@pc500.emulab.net\"\n",
    "router1_prefix = \"ssh PeterYao@pc487.emulab.net\"\n",
    "rx0_prefix = \"ssh PeterYao@pc816.emulab.net\"\n",
    "\n",
    "nodes_prefix = [tx0_prefix, router0_prefix, router1_prefix, rx0_prefix]\n",
    "\n",
    "class node:\n",
    "    def __init__(self, node_ssh_prefix) -> None:\n",
    "        self.ssh_prefix = node_ssh_prefix\n",
    "\n",
    "    def execute(self, command, background=False):\n",
    "        if background:\n",
    "            print(\"executing in background\")\n",
    "            # full_command = f\"{self.ssh_prefix} 'setsid nohup {command} > /dev/null 2>&1 &'\"\n",
    "            full_command = f'{self.ssh_prefix} \"{command}\"'\n",
    "            subprocess.Popen(full_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        else:\n",
    "            full_command = f'{self.ssh_prefix} \"{command}\"'\n",
    "            result = subprocess.run(full_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            if result.returncode == 0:\n",
    "                print(result.stdout.decode('utf-8'))\n",
    "            else:\n",
    "                print(f\"Error: {result.stderr.decode('utf-8')}\")\n",
    "        return None\n",
    "        \n",
    "tx0_node = node(tx0_prefix)\n",
    "delay_node = node(router0_prefix)\n",
    "router_node = node(router1_prefix)\n",
    "rx0_node = node(rx0_prefix)\n",
    "\n",
    "nodes = [tx0_node, delay_node, router_node, rx0_node]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabric import Connection\n",
    "\n",
    "\n",
    "tx = Connection(\n",
    "    host='pc490.emulab.net',\n",
    "    user = 'PeterYao',\n",
    "    port=22,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "delay = Connection(\n",
    "    host='pc500.emulab.net',\n",
    "    user = 'PeterYao',\n",
    "    port=22,\n",
    ")\n",
    "\n",
    "\n",
    "router = Connection(\n",
    "    host='pc487.emulab.net',\n",
    "    user='PeterYao',\n",
    "    port=22,\n",
    ")\n",
    "\n",
    "rx = Connection(\n",
    "    host='pc816.emulab.net',\n",
    "    user\n",
    "    = 'PeterYao',  \n",
    "    port=22,\n",
    ")\n",
    "\n",
    "conns = [router, delay, tx, rx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 50-50 base thp and latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands_noecn = \"bash -c 'sudo sysctl -w net.ipv4.tcp_congestion_control=cubic; sudo sysctl -w net.ipv4.tcp_ecn=0'\"\n",
    "for node in nodes:\n",
    "    node.execute(commands_noecn)\n",
    "    \n",
    "print(\"validating...\")\n",
    "for node in nodes:\n",
    "    node.execute(\"sudo sysctl net.ipv4.tcp_congestion_control\")\n",
    "    node.execute(\"sudo sysctl net.ipv4.tcp_ecn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason, this cannot be automated\n",
    "\n",
    "for node in nodes:\n",
    "    # Download and unzip the kernel package\n",
    "    node.execute(\"wget https://github.com/L4STeam/linux/releases/download/testing-build/l4s-testing.zip\")\n",
    "    node.execute(\"sudo apt install unzip\")\n",
    "    node.execute(\"unzip l4s-testing.zip\")\n",
    "    \n",
    "    # Install the kernel packages and update GRUB\n",
    "    node.execute(\"sudo dpkg --install debian_build/*\")\n",
    "    node.execute(\"sudo update-grub\")\n",
    "    node.execute(\"sudo reboot\")\n",
    "\n",
    "for node in nodes:\n",
    "    # check kernel version\n",
    "    node.execute(\"hostname; uname -a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_dualpi2=\"\"\"sudo apt-get update\n",
    "sudo apt -y install git gcc make bison flex libdb-dev libelf-dev pkg-config libbpf-dev libmnl-dev libcap-dev libatm1-dev selinux-utils libselinux1-dev\n",
    "sudo git clone https://github.com/L4STeam/iproute2.git\n",
    "cd iproute2\n",
    "sudo chmod +x configure\n",
    "sudo ./configure\n",
    "sudo make\n",
    "sudo make install\"\"\"\n",
    "\n",
    "router.run(cmd_dualpi2)\n",
    "router.run(\"sudo modprobe sch_dualpi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = ['iperf3', 'net-tools', 'moreutils']\n",
    "for conn in conns:\n",
    "    for package in packages:\n",
    "        conn.sudo(f'sudo apt update; apt-get -y install {package}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offloads = [\"gro\", \"lro\", \"gso\", \"tso\"]\n",
    "\n",
    "for conn in conns[:3]:\n",
    "    for offload in offloads:\n",
    "        conn.sudo(f'ethtool -K eno3 {offload} off', warn=True)\n",
    "        conn.sudo(f'ethtool -K enp5s0f0 {offload} off', warn=True)\n",
    "        conn.sudo(f'ethtool -K enp4s0f0 {offload} off',     warn=True)\n",
    "        conn.sudo(f'ethtool -K enp6s0f3 {offload} off',    warn=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx.sudo('ethtool -K vlan171 gro off')\n",
    "rx.sudo('ethtool -K vlan171 lro off')\n",
    "rx.sudo('ethtool -K vlan171 gso off')\n",
    "rx.sudo('ethtool -K vlan171 tso off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the delay on the delay node\n",
    "base_rtt = 25\n",
    "delay_interfaces = [\"enp5s0f0\", \"eno3\"]\n",
    "\n",
    "for e in delay_interfaces:\n",
    "    cmds = \"sudo tc qdisc replace dev {iface} root netem delay {owd}ms limit 60000\".format(iface=e, owd=base_rtt/2)\n",
    "    delay.run(cmds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay.run(\"sudo tc qdisc show dev enp5s0f0\")\n",
    "delay.run(\"sudo tc qdisc show dev eno3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "btl limit:  625000\n",
      "packet number:  417\n"
     ]
    }
   ],
   "source": [
    "# set up the btl node\n",
    "n_bdp = 2\n",
    "base_rtt = 25\n",
    "btl_capacity = 100 #in Mbps\n",
    "\n",
    "# fixed values\n",
    "btl_limit    = int(1000*n_bdp*btl_capacity*base_rtt/8) # limit of the bottleneck, n_bdp x BDP in bytes \n",
    "packet_number=int(btl_limit/1500)+1\n",
    "\n",
    "print(\"btl limit: \", btl_limit)\n",
    "print(\"packet number: \", packet_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabric import ThreadingGroup\n",
    "from invoke.exceptions import UnexpectedExit\n",
    "\n",
    "# Define hosts with respective ports\n",
    "hosts = [\n",
    "    'PeterYao@pc605.emulab.net:30042',\n",
    "    'PeterYao@pc603.emulab.net:30042',\n",
    "    'PeterYao@pc604.emulab.net:30042',\n",
    "    'PeterYao@pc760.emulab.net:22',\n",
    "]\n",
    "\n",
    "# Initialize the ThreadingGroup\n",
    "group = ThreadingGroup(*hosts)\n",
    "\n",
    "commands = [\n",
    "    \"wget https://github.com/L4STeam/linux/releases/download/testing-build/l4s-testing.zip\",\n",
    "    \"sudo apt update && sudo apt install -y unzip\",\n",
    "    \"unzip l4s-testing.zip\",\n",
    "    \"sudo dpkg --install debian_build/*\",\n",
    "    \"sudo update-grub\",\n",
    "    \"sudo reboot\"\n",
    "]\n",
    "\n",
    "with group as g:\n",
    "    for cmd in commands:\n",
    "        try:\n",
    "            print(f\"Executing: {cmd}\")\n",
    "            g.run(cmd, hide=False)\n",
    "        except UnexpectedExit as e:\n",
    "            print(f\"Command failed: {cmd}\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"hostname; uname -a\"\n",
    "\n",
    "with group as g:\n",
    "    try:\n",
    "        print(f\"Executing: {cmd}\")\n",
    "        g.run(cmd, hide=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Command failed: {cmd}\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the router queueing discipline\n",
    "router_egress_name = \"eno3\"\n",
    "\n",
    "cmds_prefix = '''\n",
    "            sudo tc qdisc del dev {iface} root\n",
    "            sudo tc qdisc replace dev {iface} root handle 1: htb default 3 \n",
    "            sudo tc class add dev {iface} parent 1: classid 1:3 htb rate {capacity}mbit \n",
    "            '''.format(iface=router_egress_name, capacity=btl_capacity, buffer=btl_limit)\n",
    "            \n",
    "cmds_specific = \"sudo tc qdisc add dev {iface} parent 1:3 handle 3: bfifo limit {buffer}\".format(iface=router_egress_name, buffer=btl_limit)\n",
    "\n",
    "router.run(cmds_prefix)    \n",
    "router.run(cmds_specific)\n",
    "router.run(\"sudo tc qdisc show dev {iface}\".format(iface=router_egress_name))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_egress_name = \"eno3\"\n",
    "\n",
    "router.run(\"sudo tc qdisc show dev {iface}\".format(iface=router_egress_name))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx.sudo(\"ip netns exec ue1 sysctl -w net.ipv4.tcp_congestion_control=cubic\")\n",
    "rx.sudo(\"ip netns exec ue3 sysctl -w net.ipv4.tcp_congestion_control=cubic\")\n",
    "\n",
    "rx.sudo(\"ip netns exec ue1 sysctl -w net.ipv4.tcp_ecn=0\")\n",
    "rx.sudo(\"ip netns exec ue3 sysctl -w net.ipv4.tcp_ecn=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx.sudo(\"docker exec rfsim5g-oai-nr-ue1 sysctl net.ipv4.tcp_congestion_control\")\n",
    "rx.sudo(\"docker exec rfsim5g-oai-nr-ue1 sysctl -w net.ipv4.tcp_ecn=0\")\n",
    "rx.sudo(\"docker exec rfsim5g-oai-nr-ue1 sysctl net.ipv4.tcp_ecn\")\n",
    "\n",
    "\n",
    "rx.sudo(\"docker exec rfsim5g-oai-nr-ue2 sysctl net.ipv4.tcp_congestion_control\")\n",
    "rx.sudo(\"docker exec rfsim5g-oai-nr-ue2 sysctl -w net.ipv4.tcp_ecn=0\")\n",
    "rx.sudo(\"docker exec rfsim5g-oai-nr-ue2 sysctl net.ipv4.tcp_ecn\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx.run(\"ls -l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in conns:\n",
    "    c.run(\"sudo sysctl -w net.ipv4.tcp_no_metrics_save=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rx.sudo(\"docker exec -it rfsim5g-oai-nr-ue1 apt-get update\")\n",
    "# rx.sudo(\"docker exec -it rfsim5g-oai-nr-ue1 apt-get install -y psmisc\")\n",
    "rx.sudo(\"docker exec -it rfsim5g-oai-nr-ue2 apt-get update\", warn=True)\n",
    "rx.sudo(\"docker exec -it rfsim5g-oai-nr-ue2 apt-get install -y psmisc\", warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the iperf command\n",
    "\n",
    "\n",
    "rx.run(\"killall iperf3\", warn=True)\n",
    "rx.sudo(\"docker exec rfsim5g-oai-nr-ue1 killall iperf3\", warn=True)\n",
    "rx.sudo(\"docker exec rfsim5g-oai-nr-ue2 killall iperf3\", warn=True)\n",
    "\n",
    "router_egress_name = \"eno3\"\n",
    "router.run(\"sudo ethtool -S {iface}\".format(iface=router_egress_name))\n",
    "router.run(\"ip -s link show {iface}\".format(iface=router_egress_name))\n",
    "\n",
    "rx.sudo(\"ip netns exec ue1 iperf3 -s -1 -p 4008 -D\")\n",
    "rx.sudo(\"ip netns exec ue3 iperf3 -s -1 -p 4008 -D\")\n",
    "# rx.sudo(\"docker exec rfsim5g-oai-nr-ue1 iperf3 -s -1 -p 4008 -D\")\n",
    "# rx.sudo(\"docker exec rfsim5g-oai-nr-ue2 iperf3 -s -1 -p 4008 -D\")\n",
    "\n",
    "local_file_path = r\"d:\\5g notes\\5G-E2E-Wireless-Notes-OAI\\exp-9-15\\exp.sh\"\n",
    "\n",
    "# the monitor queue length shell script has already been copied to the router1 node\n",
    "router.run(\"nohup ./monitor.sh eno3 60 1 > monitor.log 2>&1 &\", pty=False)\n",
    "\n",
    "# the put command is funny on windows, so I copy paster the exp file manually to the tx node\n",
    "tx.run(\"chmod +x ~/exp.sh\")\n",
    "# monitor the RLC buffer. \n",
    "rx.run(\"/mydata/flexric/build/examples/xApp/c/monitor/xapp_gtp_mac_rlc_pdcp_moni > /dev/null 2>&1 &\", pty=False)\n",
    "tx.run(\"~/exp.sh cubic-ecn-none\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download the database file with the biggest timestamp (the latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_latest_xapp_db(rx, remote_dir='/tmp/', pattern='xapp_db_*'):\n",
    "    \"\"\"\n",
    "    Downloads the latest xapp_db_* file from the remote directory.\n",
    "\n",
    "    Parameters:\n",
    "    - rx: Fabric Connection object\n",
    "    - remote_dir: Directory on the remote host to search for files\n",
    "    - pattern: File pattern to match\n",
    "    - local_dir: Local directory to download the file to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: List all matching files\n",
    "        list_cmd = f\"ls {remote_dir}{pattern}\"\n",
    "        result = rx.run(list_cmd, hide=True, warn=True)\n",
    "\n",
    "        if not result.ok:\n",
    "            print(\"No files matching the pattern found.\")\n",
    "            return\n",
    "\n",
    "        files = result.stdout.strip().split('\\n')\n",
    "\n",
    "        # Step 2: Filter out -shm and -wal files\n",
    "        base_files = [f for f in files if not (f.endswith('-shm') or f.endswith('-wal'))]\n",
    "\n",
    "        if not base_files:\n",
    "            print(\"No base xapp_db_ files found (excluding -shm and -wal).\")\n",
    "            return\n",
    "\n",
    "        # Step 3: Extract timestamps and find the latest file\n",
    "        timestamp_pattern = re.compile(r'xapp_db_(\\d+)$')\n",
    "        latest_timestamp = -1\n",
    "        latest_file = None\n",
    "\n",
    "        for f in base_files:\n",
    "            match = timestamp_pattern.search(f)\n",
    "            if match:\n",
    "                timestamp = int(match.group(1))\n",
    "                if timestamp > latest_timestamp:\n",
    "                    latest_timestamp = timestamp\n",
    "                    latest_file = f\n",
    "            else:\n",
    "                print(f\"File {f} does not match the expected pattern.\")\n",
    "\n",
    "        if latest_file is None:\n",
    "            print(\"No valid timestamped xapp_db_ files found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Latest file found: {latest_file} with timestamp {latest_timestamp}\")\n",
    "\n",
    "        # Step 4: Download the latest file\n",
    "        rx.get(latest_file)\n",
    "        print(f\"Downloaded {latest_file}.\")\n",
    "        return latest_file\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to download the latest xapp_db_* file\n",
    "latest_file = download_latest_xapp_db(rx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse and Plot the database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your SQLite file (replace 'your_database_file.sqlite' with your file)\n",
    "db_file = 'xapp_db_1727534779300166'\n",
    "\n",
    "# Connect to the SQLite database file\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Fetch the list of tables in the database\n",
    "tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "print(\"Tables in the database:\", tables)\n",
    "\n",
    "table_name = 'RLC_bearer'\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "# Close the connection after extracting the data\n",
    "conn.close()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# # Save DataFrame to CSV if needed\n",
    "df.to_csv(f'{db_file}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('xapp_db_1727534779300166.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Get the unique RNTI values (UE identifiers)\n",
    "rntis = df['rnti'].unique()\n",
    "print(\"Unique RNTI values:\", rntis)\n",
    "\n",
    "# Adjust 'tstamp' to start at zero and convert to seconds\n",
    "# Since 'tstamp' is in microseconds, divide by 1,000,000 to get seconds\n",
    "df['time_sec'] = (df['tstamp'] - df['tstamp'].min()) / 1_000_000\n",
    "\n",
    "# Convert 'txbuf_occ_bytes' to Megabytes (MB)\n",
    "df['txbuf_occ_MB'] = df['txbuf_occ_bytes'] / (1024 * 1024)\n",
    "\n",
    "# Plot 'txbuf_occ_MB' over 'time_sec' for each UE\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for rnti in rntis:\n",
    "    df_ue = df[df['rnti'] == rnti]\n",
    "    plt.plot(df_ue['time_sec'], df_ue['txbuf_occ_MB'], label=f'UE {rnti}')\n",
    "\n",
    "# Add a dashed horizontal line at 2 MB to represent the RLC buffer limit\n",
    "plt.axhline(y=2, color='red', linestyle='--', label='RLC Buffer Limit (2 MB)')\n",
    "\n",
    "# Add plot title and labels\n",
    "plt.title('RLC Buffer Occupancy Over Time for Each UE')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('RLC Buffer Occupancy (MB)')\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the other Non-RLC information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.close()\n",
    "tx = Connection(\n",
    "    host='pc490.emulab.net',\n",
    "    user = 'PeterYao',\n",
    "    port=22,\n",
    ")\n",
    "tx.get(\"cubic_ecn_none-result-ue1.json\")\n",
    "tx.get(\"cubic_ecn_none-result-ue2.json\")\n",
    "tx.get(\"cubic_ecn_none-ss-ue1.txt\")\n",
    "tx.get(\"cubic_ecn_none-ss-ue2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace these with the paths to your actual iperf JSON result files\n",
    "file1 = 'cubic_ecn_none-result-ue1.json'\n",
    "file2 = 'cubic_ecn_none-result-ue2.json'\n",
    "\n",
    "def load_iperf_data(filename):\n",
    "    \"\"\"Load iperf JSON data from a file and return a pandas DataFrame.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Initialize lists to store the extracted data\n",
    "    intervals = data['intervals']\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    durations = []\n",
    "    throughputs = []\n",
    "    retransmissions = []\n",
    "    \n",
    "    # Iterate over each interval in the data\n",
    "    for interval in intervals:\n",
    "        sum_data = interval['sum']\n",
    "        start_times.append(sum_data['start'])\n",
    "        end_times.append(sum_data['end'])\n",
    "        durations.append(sum_data['seconds'])\n",
    "        # Convert throughput from bits per second to megabits per second\n",
    "        throughput_mbps = sum_data['bits_per_second'] / 1_000_000  # Divide by 1,000,000\n",
    "        throughputs.append(throughput_mbps)\n",
    "        retransmissions.append(sum_data.get('retransmits', 0))  # Retransmits might not be present in UDP tests\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Start Time': start_times,\n",
    "        'End Time': end_times,\n",
    "        'Duration': durations,\n",
    "        'Throughput (Mbps)': throughputs,\n",
    "        'Retransmissions': retransmissions\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_zero_throughput(df):\n",
    "    \"\"\"Filter out data points where throughput is zero.\"\"\"\n",
    "    return df[df['Throughput (Mbps)'] != 0].reset_index(drop=True)\n",
    "\n",
    "# Load data from both files\n",
    "df1 = load_iperf_data(file1)\n",
    "df2 = load_iperf_data(file2)\n",
    "\n",
    "# Filter out zero throughput data points for plotting throughput\n",
    "# df1_nonzero = filter_zero_throughput(df1)\n",
    "# df2_nonzero = filter_zero_throughput(df2)\n",
    "\n",
    "# Calculate overall throughput and average retransmissions for each dataset\n",
    "overall_throughput1 = df1['Throughput (Mbps)'].mean()\n",
    "average_retransmits1 = df1['Retransmissions'].mean()\n",
    "\n",
    "overall_throughput2 = df2['Throughput (Mbps)'].mean()\n",
    "average_retransmits2 = df2['Retransmissions'].mean()\n",
    "\n",
    "print(f\"UE 1 - Overall Throughput: {overall_throughput1:.2f} Mbps\")\n",
    "print(f\"UE 1 - Average Retransmissions: {average_retransmits1:.2f}\")\n",
    "\n",
    "print(f\"\\nUE 2 - Overall Throughput: {overall_throughput2:.2f} Mbps\")\n",
    "print(f\"UE 2 - Average Retransmissions: {average_retransmits2:.2f}\")\n",
    "\n",
    "# Plotting the throughputs over time (excluding zero throughput data points)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df1['Start Time'], df1['Throughput (Mbps)'], label='UE 1 Throughput', marker='o')\n",
    "plt.plot(df2['Start Time'], df2['Throughput (Mbps)'], label='UE 2 Throughput', marker='x')\n",
    "plt.title('Throughput Over Time (Excluding Zero Values)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Throughput (Mbps)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the retransmissions over time (include all data points)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df1['Start Time'], df1['Retransmissions'], label='UE 1 Retransmissions', marker='o')\n",
    "plt.plot(df2['Start Time'], df2['Retransmissions'], label='UE 2 Retransmissions', marker='x')\n",
    "plt.title('Retransmissions Over Time')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Number of Retransmissions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT: the cwnd calculation is not showing the expected result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_tx0=\"cubic_ecn_none\"\n",
    "\n",
    "\n",
    "# the csv files generated is of the following format\n",
    "# timestamp, fd, cwnd, srtt\n",
    "\n",
    "file_out_tx0_csv = name_tx0+\"-ss.csv\"\n",
    "\n",
    "for ue_id in range(1, 3):\n",
    "    print(\"Running to generate csv files \" + name_tx0)\n",
    "\n",
    "    ss_tx0_script_processing=\"\"\"\n",
    "\n",
    "    f_1={types}; \n",
    "    ue_id={ue_id};\n",
    "    rm -f ${{f_1}}-ss-${{ue_id}}.csv;\n",
    "    cat ${{f_1}}-ss-${{ue_id}}.txt | sed -e \":a; /<->$/ {{ N; s/<->\\\\n//; ba; }}\"  | grep \"iperf3\" | grep -v \"SYN-SENT\"> ${{f_1}}-ss-processed-${{ue_id}}.txt; \n",
    "    cat ${{f_1}}-ss-processed-${{ue_id}}.txt | awk '{{print $1}}' > ts-${{f_1}}-${{ue_id}}.txt; \n",
    "    cat ${{f_1}}-ss-processed-${{ue_id}}.txt | grep -oP '\\\\bcwnd:.*?(\\s|$)' | awk -F '[:,]' '{{print $2}}' | tr -d ' ' > cwnd-${{f_1}}-${{ue_id}}.txt; \n",
    "    cat ${{f_1}}-ss-processed-${{ue_id}}.txt | grep -oP '\\\\brtt:.*?(\\s|$)' | awk -F '[:,]' '{{print $2}}' | tr -d ' '  | cut -d '/' -f 1   > srtt-${{f_1}}-${{ue_id}}.txt; \n",
    "    cat ${{f_1}}-ss-processed-${{ue_id}}.txt | grep -oP '\\\\bfd=.*?(\\s|$)' | awk -F '[=,]' '{{print $2}}' | tr -d ')' | tr -d ' '   > fd-${{f_1}}-${{ue_id}}.txt;\n",
    "    paste ts-${{f_1}}-${{ue_id}}.txt fd-${{f_1}}-${{ue_id}}.txt cwnd-${{f_1}}-${{ue_id}}.txt srtt-${{f_1}}-${{ue_id}}.txt -d ',' > ${{f_1}}-ss-${{ue_id}}.csv;\"\"\".format(types=name_tx0, ue_id=\"ue\"+str(ue_id))\n",
    "\n",
    "    tx.run(ss_tx0_script_processing)\n",
    "\n",
    "tx.get(\"cubic_ecn_none\"+\"-ss-ue1.csv\")\n",
    "tx.get(\"cubic_ecn_none\"+\"-ss-ue2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "throughput_data = {}\n",
    "srtt_data = {}\n",
    "\n",
    "# Dictionaries to hold data for each UE\n",
    "cwnd_data = {}\n",
    "srtt_data_time = {}\n",
    "\n",
    "for ue_id in range(1, 3):\n",
    "    name_tx0 = \"cubic_ecn_none\"\n",
    "    ue_str = \"ue\" + str(ue_id)\n",
    "\n",
    "    # Load the JSON output file into a Python object\n",
    "    with open(f\"{name_tx0}-result-{ue_str}.json\") as f:\n",
    "        iperf3_data = json.load(f)\n",
    "\n",
    "    throughput_data[name_tx0 + ue_str] = iperf3_data['end']['sum_received']['bits_per_second'] / (1000000 * 1)  # to convert Mbit\n",
    "\n",
    "    # Average SRTT for Each Flow\n",
    "    columns = ['timestamp', 'flow ID', 'cwnd', 'srtt']\n",
    "    df_f1 = pd.read_csv(f\"{name_tx0}-ss-{ue_str}.csv\", names=columns)\n",
    "\n",
    "    # Filter out rows with flow ID = 4, they are for the control flows\n",
    "    df_f1 = df_f1[df_f1['flow ID'] != 4].reset_index(drop=True)\n",
    "\n",
    "    average_RTT_f1 = df_f1['srtt'].mean()\n",
    "\n",
    "    # Normalize the timestamps to start from zero\n",
    "    df_f1['timestamp'] = df_f1['timestamp'] - df_f1['timestamp'].iloc[0]\n",
    "\n",
    "    # Save cwnd and srtt data into separate dataframes for each UE\n",
    "    cwnd_data[ue_str] = df_f1[['timestamp', 'cwnd']]\n",
    "    srtt_data_time[ue_str] = df_f1[['timestamp', 'srtt']]\n",
    "\n",
    "    srtt_data[name_tx0 + ue_str] = average_RTT_f1\n",
    "\n",
    "# Save throughput_data to a JSON file\n",
    "with open('throughput_data.json', 'w') as f:\n",
    "    json.dump(throughput_data, f)\n",
    "\n",
    "# Save srtt_data to a JSON file\n",
    "with open('srtt_data.json', 'w') as f:\n",
    "    json.dump(srtt_data, f)\n",
    "\n",
    "# Save cwnd_data and srtt_data_time to CSV files for each UE\n",
    "for ue_str in cwnd_data.keys():\n",
    "    cwnd_data[ue_str].to_csv(f\"cwnd_data_{ue_str}.csv\", index=False)\n",
    "    srtt_data_time[ue_str].to_csv(f\"srtt_data_time_{ue_str}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to hold data\n",
    "timestamps = []\n",
    "cwnds = []\n",
    "ue_ids = []\n",
    "\n",
    "# Loop through UEs to read their data\n",
    "for ue_id in range(1, 3):\n",
    "    ue_str = \"ue\" + str(ue_id)\n",
    "    filename = f'cwnd_data_{ue_str}.csv'\n",
    "    \n",
    "    # Read the CSV file for the UE\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Append data to lists\n",
    "    timestamps.append(df['timestamp'])\n",
    "    cwnds.append(df['cwnd'] * 1448)  # Convert cwnd to bytes\n",
    "    ue_ids.append(ue_str)\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "\n",
    "# Plot for each UE\n",
    "for i, ue_str in enumerate(ue_ids):\n",
    "    axes[i].plot(timestamps[i], cwnds[i], marker='', label=ue_str)\n",
    "    axes[i].set_title(f'Congestion Window (cwnd) Over Time - {ue_str.upper()}')\n",
    "    axes[i].set_xlabel('Time (seconds)')\n",
    "    axes[i].set_ylabel('cwnd (bytes)')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router.get(\"monitor.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace with your actual filename\n",
    "filename = 'monitor.log'\n",
    "\n",
    "def parse_qdisc_data(filename):\n",
    "    \"\"\"Parse qdisc data from a file and return a DataFrame.\"\"\"\n",
    "    timestamps = []\n",
    "    sent_bytes = []\n",
    "    sent_packets = []\n",
    "    dropped_packets = []\n",
    "    overlimits = []\n",
    "    backlog_bytes = []\n",
    "    backlog_packets = []\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # Extract timestamp\n",
    "            match_time = re.match(r'^(\\d+\\.\\d+)', line)\n",
    "            if match_time:\n",
    "                timestamp = float(match_time.group(1))\n",
    "                timestamps.append(timestamp)\n",
    "            else:\n",
    "                continue  # Skip line if timestamp not found\n",
    "\n",
    "            # Extract bfifo qdisc statistics\n",
    "            # Assuming 'qdisc bfifo' appears after 'qdisc htb' in the line\n",
    "            bfifo_data = line.split('qdisc bfifo')[1]\n",
    "\n",
    "            # Extract Sent bytes and packets\n",
    "            match_sent = re.search(r'Sent\\s+(\\d+)\\s+bytes\\s+(\\d+)\\s+pkt', bfifo_data)\n",
    "            if match_sent:\n",
    "                sent_bytes.append(int(match_sent.group(1)))\n",
    "                sent_packets.append(int(match_sent.group(2)))\n",
    "            else:\n",
    "                sent_bytes.append(None)\n",
    "                sent_packets.append(None)\n",
    "\n",
    "            # Extract dropped packets\n",
    "            match_dropped = re.search(r'dropped\\s+(\\d+)', bfifo_data)\n",
    "            if match_dropped:\n",
    "                dropped_packets.append(int(match_dropped.group(1)))\n",
    "            else:\n",
    "                dropped_packets.append(None)\n",
    "\n",
    "            # Extract overlimits\n",
    "            match_overlimits = re.search(r'overlimits\\s+(\\d+)', bfifo_data)\n",
    "            if match_overlimits:\n",
    "                overlimits.append(int(match_overlimits.group(1)))\n",
    "            else:\n",
    "                overlimits.append(None)\n",
    "\n",
    "            # Extract backlog bytes and packets\n",
    "            match_backlog = re.search(r'backlog\\s+(\\d+)b\\s+(\\d+)p', bfifo_data)\n",
    "            if match_backlog:\n",
    "                backlog_bytes.append(int(match_backlog.group(1)))\n",
    "                backlog_packets.append(int(match_backlog.group(2)))\n",
    "            else:\n",
    "                backlog_bytes.append(None)\n",
    "                backlog_packets.append(None)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Timestamp': timestamps,\n",
    "        'Sent Bytes': sent_bytes,\n",
    "        'Sent Packets': sent_packets,\n",
    "        'Dropped Packets': dropped_packets,\n",
    "        'Overlimits': overlimits,\n",
    "        'Backlog Bytes': backlog_bytes,\n",
    "        'Backlog Packets': backlog_packets\n",
    "    })\n",
    "\n",
    "    # Convert timestamps to relative time (seconds since start)\n",
    "    df['Relative Time'] = df['Timestamp'] - df['Timestamp'].iloc[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_queue_length(df):\n",
    "    \"\"\"Plot queue length (backlog) over time.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Relative Time'], df['Backlog Bytes'], marker='o')\n",
    "    plt.title('Queue Length (Backlog) Over Time')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Backlog (bytes)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_packet_drop_rate(df):\n",
    "    \"\"\"Plot packet drop rate over time.\"\"\"\n",
    "    # Calculate the difference in dropped packets between measurements\n",
    "    df['Dropped Packets Diff'] = df['Dropped Packets'].diff().fillna(0)\n",
    "    \n",
    "    print(df['Dropped Packets Diff'])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Relative Time'], df['Dropped Packets Diff'], marker='x', color='red')\n",
    "    plt.title('Packet Drop Rate Over Time')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Dropped Packets per Interval')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def analyze_backlog(df):\n",
    "    \"\"\"Analyze backlog and determine if there is a backlog.\"\"\"\n",
    "    max_backlog = df['Backlog Bytes'].max()\n",
    "    if max_backlog > 0:\n",
    "        print(f\"There is a backlog. Maximum backlog is {max_backlog} bytes.\")\n",
    "    else:\n",
    "        print(\"There is no backlog. The queue is empty throughout the measurements.\")\n",
    "\n",
    "# Main execution\n",
    "df = parse_qdisc_data(filename)\n",
    "\n",
    "# Plot queue length\n",
    "plot_queue_length(df)\n",
    "\n",
    "# Plot packet drop rate\n",
    "plot_packet_drop_rate(df)\n",
    "\n",
    "# Analyze backlog\n",
    "analyze_backlog(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the SRTT data for each UE\n",
    "ue1_file_path = 'srtt_data_time_ue1.csv'  # Replace with your actual filename for UE1\n",
    "ue2_file_path = 'srtt_data_time_ue2.csv'  # Replace with your actual filename for UE2\n",
    "\n",
    "# Reading the CSV files\n",
    "df_ue1 = pd.read_csv(ue1_file_path)\n",
    "df_ue2 = pd.read_csv(ue2_file_path)\n",
    "\n",
    "# Normalize timestamps to start from zero for better comparison\n",
    "df_ue1['timestamp'] = df_ue1['timestamp'] - df_ue1['timestamp'].iloc[0]\n",
    "df_ue2['timestamp'] = df_ue2['timestamp'] - df_ue2['timestamp'].iloc[0]\n",
    "\n",
    "# Calculate the average SRTT for each UE\n",
    "avg_ue1 = df_ue1['srtt'].mean()\n",
    "avg_ue2 = df_ue2['srtt'].mean()\n",
    "\n",
    "# Plot the SRTT changes over time for both UE1 and UE2\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_ue1['timestamp'], df_ue1['srtt'], label='UE1 SRTT', marker='o')\n",
    "plt.plot(df_ue2['timestamp'], df_ue2['srtt'], label='UE2 SRTT', marker='s')\n",
    "\n",
    "# Plot average lines for each UE\n",
    "plt.axhline(y=avg_ue1, color='blue', linestyle='--', linewidth=1, label=f'Average UE1 ({avg_ue1:.2f} ms)')\n",
    "plt.axhline(y=avg_ue2, color='orange', linestyle='--', linewidth=1, label=f'Average UE2 ({avg_ue2:.2f} ms)')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('SRTT (ms)')\n",
    "plt.title('SRTT Changes Over Time for UE1 and UE2')\n",
    "\n",
    "# Adding a legend and grid\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the average SRTT for each UE\n",
    "print(f\"Average SRTT for UE1: {avg_ue1:.3f} ms\")\n",
    "print(f\"Average SRTT for UE2: {avg_ue2:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
